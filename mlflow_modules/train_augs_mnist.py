# -*- coding: utf-8 -*-
"""train_MNIST.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HdEAVRqWiGsljjCO_QalK0OxKxYS2-XD
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import torchvision.datasets as dsets
import torchvision.transforms as transforms
import torchvision.transforms.v2 as transforms_v2
import numpy as np
import matplotlib.pyplot as plt
import torch.optim as optim
import torch.nn.init as init
import time
import os
from datetime import datetime
from google.colab import files

from model.vitbnv4 import ViTBN
from emnist_digit_preprocessing import download_emnist
from emnist_digit_preprocessing import MNISTCustomDataset

import mlflow

mlflow.set_tracking_uri(os.getenv("MLFLOW_TRACKING_URI"))

#from dagshub import dagshub_logger

#mlflow.set_tracking_uri("https://dagshub.com/anindyahepth/BatchNorm_in_Transformers_CV")
#os.environ['MLFLOW_TRACKING_USERNAME'] = USER_NAME
#os.environ['MLFLOW_TRACKING_PASSWORD'] = PASSWORD

def get_datasets_emnist() :
  dir_root = '/content/'
  file_dict={
    'train_images':'emnist-digits-train-images-idx3-ubyte.gz',
    'train_labels':'emnist-digits-train-labels-idx1-ubyte.gz',
    'test_images':'emnist-digits-test-images-idx3-ubyte.gz',
    'test_labels':'emnist-digits-test-labels-idx1-ubyte.gz'
  }
  dataset = download_emnist(dir_root,file_dict)

  train_images=dataset[0]
  train_labels=dataset[1]
  test_images=dataset[2]
  test_labels=dataset[3]

  data_transform = transforms.Compose([
    transforms.ToPILImage(),
    lambda img: torchvision.transforms.functional.rotate(img, -90),
    lambda img: torchvision.transforms.functional.hflip(img),
    transforms.RandomAffine(degrees = 0, translate = (0.2, 0.2)),
    transforms_v2.RandomZoomOut(0,(2.0, 2.0), p=0.2),
    transforms.Resize(28),
    transforms.ToTensor()
    ]
)

#training_dataset

  train_dataset = MNISTCustomDataset(train_images, train_labels, transform=data_transform, label_type='integer')

#validation_dataset

  validation_dataset = MNISTCustomDataset(test_images, test_labels, transform=data_transform, label_type='integer')

  return train_dataset, validation_dataset


def get_datasets_mnist() :
  data_transform = transforms.Compose([
    transforms.RandomRotation(20),
    transforms.RandomAffine(degrees = 0, translate = (0.3, 0.3)),
    transforms_v2.RandomZoomOut(0,(2.0, 2.0), p=0.4),
    transforms.Resize(28),
    transforms.ToTensor()
    ])

  #Load training data

  train_dataset = dsets.MNIST(root='./data', train=True, download=True, transform= data_transform)

  #Load validation data
  validation_dataset = dsets.MNIST(root='./data', train=False, download=True, transform= data_transform)

  return train_dataset, validation_dataset


def train_model(model,train_loader,validation_loader, train_dataset, validation_dataset, optimizer,criterion,n_epochs):

    #global variable
    N_test=len(validation_dataset)	
    N_train=len(train_dataset)
    accuracy_train_list=[]
    accuracy_list=[]
    cost_list=[]
    cost_test_list=[]
    loss_list=[]
    dur_list_train=[]
    dur_list_val = []
    class_correct = [[0.for i in range(10)] for j in range(n_epochs)]
    class_total= [[0.for i in range (10)] for j in range(n_epochs)]
    class_accuracy =[[0.for i in range(10)] for j in range(n_epochs)]
    COST=0.
    COST_test=0.
    correct_train=0.
    correct=0.
    delta_train=0
    delta_val=0

    for epoch in range(n_epochs):
        COST=0.
	correct_train=0
        t0 = datetime.now()
        for x, y in train_loader:
            optimizer.zero_grad()
            model.train()
            z = model(x)
            loss = criterion(z, y)
            loss.backward()
            optimizer.step()
            COST+=loss.data.item()
	    _, yhat = torch.max(z, 1)
            correct_train += (yhat == y).sum().item()
            #loss_list.append(loss.data)
        cost_list.append(COST)
	accuracy_train_list= correct_train/N_train
        delta_train= datetime.now() - t0
        dur_list_train.append(delta_train.total_seconds())

        correct = 0.
	COST_test=0.
        t1 = datetime.now()
        #perform a prediction on the validation  data
        for x_test, y_test in validation_loader:
            model.eval()
            z = model(x_test)
            _, yhat = torch.max(z, 1)
            correct += (yhat == y_test).sum().item()
	    loss = criterion(z, y_test)
	    COST_test +=loss.data.item() 
		
		
            b = y_test.shape[0]

            for i in range(b):
                if yhat[i] == y_test[i] :
                  class_correct[epoch][y_test[i].item()] += 1
                  class_total[epoch][y_test[i].item()] += 1
                else:
                  class_correct[epoch][y_test[i].item()] += 0
                  class_total[epoch][y_test[i].item()] += 1

        for i in range(10):
             if class_total[epoch][i] > 0:
                class_accuracy[epoch][i] = 100 * class_correct[epoch][i] / class_total[epoch][i]
             else:
                class_accuracy[epoch][i] = 0
        accuracy = correct / N_test
        accuracy_list.append(accuracy)
        delta_val=datetime.now() - t1
        dur_list_val.append(delta_val.total_seconds())
	cost_test_list.append(COST_test)

    return cost_list, accuracy_list, dur_list_train, dur_list_val, class_accuracy, accuracy_train_list, cost_test_list





def get_model():
  model = ViTBN(
                image_size = 28,
                patch_size = 7,
                num_classes = 10,
                channels =1,
                dim = 64,
                depth = 6,
                heads = 8,
                mlp_dim = 128,
                pool = 'cls',
                dropout = 0.0,
                emb_dropout = 0.0,
                pos_emb ='learn'
    )

  model.load_state_dict(torch.load("ViTBN_aug_50_fin.pth"))

  return model





if __name__ == "__main__":

    mlflow.pytorch.autolog()


    learning_rate = 0.001
    n_epochs = 1
    batch_size = 100
    criterion = nn.CrossEntropyLoss()
    

    with mlflow.start_run(experiment_id=):
        train_dataset, validation_dataset = get_datasets_emnist()
        model = get_model()
        #logged_model = 'runs:/6f43f730540040b1a28fbcce66b40dc3/model_mnist'
        #model = mlflow.pytorch.load_model(logged_model)
        
        
        optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)
        train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size,shuffle=True)
        validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=5000, shuffle=True)
        
        
        cost_list, accuracy_list, dur_list_train, dur_list_val, class_accuracy, accuracy_train_list, cost_test_list = train_model(model,train_loader,validation_loader, train_dataset, validation_dataset,optimizer,criterion,n_epochs)

		

      #  with dagshub_logger() as logger:
      #      logger.log_metrics(loss_tr=cost_list, accuracy_val=accuracy_list, time_tr = dur_list_train, time_val=dur_list_val)
      #     logger.log_hyperparams({
      #         "learning_rate": learning_rate,
      #         "epochs": n_epochs
      #      })
      
      
        
        mlflow.log_params({
            "learning_rate": learning_rate,
	    "batch_size" : batch_size,
            "epochs": n_epochs
        })
        
        #mlflow.pytorch.log_model(model,"model_mnist")
        
        for i in range(n_epochs):
        	mlflow.log_metrics(
            {
                "training_loss": cost_list[i],
                "validation_accuracy": accuracy_list[i],
                "training_time": dur_list_train[i],
                "validation_time": dur_list_val[i],
		"training_accuracy":accuracy_train_list[i],
		"test_loss": cost_test_list[i]
            },step = i+1
        )

        #print("Saving the model...")
        
torch.save(model.state_dict(), 'model.pth')

# download checkpoint file

files.download('model.pth')     

print("done.")
