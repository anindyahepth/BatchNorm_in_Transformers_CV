
Batch Normalization is a powerful regularization method for Neural Networks which can 
substantially speed up the training process by allowing a higher learning rate and 
removing dropout. We investigate the impact of Batch Normalization in Transformer-based 
models tailored to Computer Vision tasks. As a first step, we implement a batchnorm layer 
in the Feed Forward Netweork (FFN) component of a Vision Transformer (ViT). 





https://anindyadey.pythonanywhere.com/
