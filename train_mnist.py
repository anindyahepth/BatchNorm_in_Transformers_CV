# -*- coding: utf-8 -*-
"""train_MNIST.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HdEAVRqWiGsljjCO_QalK0OxKxYS2-XD
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import torchvision.datasets as dsets
import torchvision.transforms as transforms
import numpy as np
import matplotlib.pyplot as plt
import torch.optim as optim
import torch.nn.init as init
import time
import os
from datetime import datetime
from google.colab import files

from model.vitbnv1a import ViTBN

import mlflow

mlflow.set_tracking_uri(os.getenv("MLFLOW_TRACKING_URI"))

#from dagshub import dagshub_logger

#mlflow.set_tracking_uri("https://dagshub.com/anindyahepth/BatchNorm_in_Transformers_CV")
#os.environ['MLFLOW_TRACKING_USERNAME'] = USER_NAME
#os.environ['MLFLOW_TRACKING_PASSWORD'] = PASSWORD




def get_datasets() :
  data_transform = transforms.Compose([
    transforms.Resize(28),
    transforms.ToTensor()
    ])

  #Load training data

  train_dataset = dsets.MNIST(root='./data', train=True, download=True, transform= data_transform)

  #Load validation data
  validation_dataset = dsets.MNIST(root='./data', train=False, download=True, transform= data_transform)

  return train_dataset, validation_dataset


def train_model(model,train_loader,validation_loader, validation_dataset, optimizer,criterion,n_epochs):

    #global variable
    N_test=len(validation_dataset)
    accuracy_list=[]
    cost_list=[]
    loss_list=[]
    dur_list_train=[]
    dur_list_val = []
    class_correct = [[0.for i in range(10)] for j in range(n_epochs)]
    class_total= [[0.for i in range (10)] for j in range(n_epochs)]
    class_accuracy =[[0.for i in range(10)] for j in range(n_epochs)]
    COST=0.
    correct=0.
    delta_train=0
    delta_val=0

    for epoch in range(n_epochs):
        COST=0.
        t0 = datetime.now()
        for x, y in train_loader:
            optimizer.zero_grad()
            model.train()
            z = model(x)
            loss = criterion(z, y)
            loss.backward()
            optimizer.step()
            COST+=loss.data.item()
            #loss_list.append(loss.data)
        cost_list.append(COST)
        delta_train= datetime.now() - t0
        dur_list_train.append(delta_train.total_seconds())

        correct = 0.
        t1 = datetime.now()
        #perform a prediction on the validation  data
        for x_test, y_test in validation_loader:
            model.eval()
            z = model(x_test)
            _, yhat = torch.max(z, 1)
            correct += (yhat == y_test).sum().item()

            b = y_test.shape[0]

            for i in range(b):
                if yhat[i] == y_test[i] :
                  class_correct[epoch][y_test[i].item()] += 1
                  class_total[epoch][y_test[i].item()] += 1
                else:
                  class_correct[epoch][y_test[i].item()] += 0
                  class_total[epoch][y_test[i].item()] += 1

        for i in range(10):
             if class_total[epoch][i] > 0:
                class_accuracy[epoch][i] = 100 * class_correct[epoch][i] / class_total[epoch][i]
             else:
                class_accuracy[epoch][i] = 0
        accuracy = correct / N_test
        accuracy_list.append(accuracy)
        delta_val=datetime.now() - t1
        dur_list_val.append(delta_val.total_seconds())

    return cost_list, accuracy_list, dur_list_train, dur_list_val, class_accuracy





def get_model():
  model = ViTBN(
                image_size = 28,
                patch_size = 7,
                num_classes = 10,
                channels =1,
                dim = 64,
                depth = 6,
                heads = 8,
                mlp_dim = 128,
                pool = 'cls',
                dropout = 0.0,
                emb_dropout = 0.0,
                pos_emb ='learn'
    )

  #model.load_state_dict(torch.load("model100epoch_mnist.pth"))

  return model





if __name__ == "__main__":

    mlflow.pytorch.autolog()


    learning_rate = 0.001
    n_epochs = 1
    criterion = nn.CrossEntropyLoss()
    

    with mlflow.start_run(experiment_id=):
        train_dataset, validation_dataset = get_datasets()
        model = get_model()
        
        optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)
        train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100,shuffle=True)
        validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=5000, shuffle=True)
        
        
        cost_list, accuracy_list, dur_list_train, dur_list_val, class_accuracy = train_model(model,train_loader,validation_loader,validation_dataset,optimizer,criterion,n_epochs)

		

      #  with dagshub_logger() as logger:
      #      logger.log_metrics(loss_tr=cost_list, accuracy_val=accuracy_list, time_tr = dur_list_train, time_val=dur_list_val)
      #     logger.log_hyperparams({
      #         "learning_rate": learning_rate,
      #         "epochs": n_epochs
      #      })
      
      
        
        mlflow.log_params({
            "learning_rate": learning_rate,
            "epochs": n_epochs
        })
        
        for i in range(n_epochs):
        	mlflow.log_metrics(
            {
                "training_loss": cost_list[i],
                "validation_accuracy": accuracy_list[i],
                "training_time": dur_list_train[i],
                "validation_time": dur_list_val[i]
            },step = i+1
        )

        #print("Saving the model...")
        
torch.save(model.state_dict(), 'model_mnist.pth')

mlflow.log_artifact('model_mnist.pth')

print("done.")